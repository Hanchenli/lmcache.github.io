---
layout: post
title: "Deploying LLMs in Clusters #2: running “vLLM production-stack” on AWS EKS and GCP GKE"
thumbnail-img: /assets/img/cloud_logo.png
share-img: /assets/img/cloud_logo.png
author: LMCache Team
image: /assets/img/cloud_logo.png
---
<br>


## TL;DR
- vLLM boasts the largest open-source community in LLM serving, and **“vLLM production-stack”** offers a vLLM-based full inference stack with **10x** better performance and Easy cluster **management**.
- Today, we will give a step-by-step demonstration on how to deploy **“vLLM production-stack”** across multiple nodes in AWS EKS and GCP.
- As per the poll result last week, next week's topic will be manageing multiple models within a single Kubernetes cluster. Tell us what is next: [[poll]](https://forms.gle/QdBhpCPzv8tMP7QJA)

[[Github Link]](https://github.com/vllm-project/production-stack) | [[More Tutorials]](https://github.com/vllm-project/production-stack/tree/main/tutorials) | [[Interest Form]](https://forms.gle/Jaq2UUFjgvuedRPV8)

#### [**AWS Tutorial** (click here)](https://github.com/vllm-project/production-stack/blob/main/tutorials/cloud_deployments/01-AWS-EKS-deployment.md)

#### [**GKE Tutorial** (click here)](https://github.com/vllm-project/production-stack/blob/main/tutorials/cloud_deployments/02-GCP-GKE-deployment.md)

# The Context
<!-- Over the past year, LLM inference has raced to the forefront, powering everything from chatbots to code assistants and beyond. It’s quickly becoming critical infrastructure, much like the cloud was to big data, cellular was to mobile apps, and CDNs were (and still are!) to the broader Internet. -->


**vLLM** has taken the open-source community by storm, with unparalleled hardware and model support plus an active ecosystem of top-notch contributors. But until now, vLLM has mostly focused on **single-node** deployments.

**vLLM Production-stack** is an open-source **reference implementation** of an **inference stack** built on top of vLLM, designed to run seamlessly on a cluster of GPU nodes. It adds four critical functionalities that complement vLLM’s native strengths.

vLLM production-stack offers superior performance than other LLM serving solutions by achieving higher throughput through smart routing and KV cache sharing\:
<div align="center">
<img src="/assets/img/stack-ttft.png" alt="Icon" style="width: 50%; vertical-align:middle;">
</div>


# Deploying a minimal vLLM Production-Stack Demo in the EKS and GCP

Setting up a vLLM production-stack in the cloud requires deploying Kubernetes clusters with GPU support, configuring persistent volume for the cluster, and deploying the inference stack using Helm. Below is an overview of how to get started with minimal configurations for both AWS EKS and GCP GKE.

## Deploying on AWS with EKS
To run the service, go into the "deployment_on_cloud/aws" folder and run:

```bash
bash entry_point.sh YOUR_AWSREGION EXAMPLE_YAML_PATH
```

Expected output:
```plaintext
NAME                                            READY   STATUS    RESTARTS   AGE
vllm-deployment-router-69b7f9748d-xrkvn         1/1     Running   0          75s
vllm-llama8b-deployment-vllm-696c998c6f-mvhg4   1/1     Running   0          75s
vllm-llama8b-deployment-vllm-696c298d9s-fdhw4   1/1     Running   0          75s

```

Clean up the service (not including VPC) with:

```bash
bash clean_up.sh production-stack YOUR_AWSREGION
```

The script will:
1. Set up an EKS cluster with GPU nodes.

2. Configure Amazon EFS for future use as persistent volume.

3. Configure Security groups for EFS and allow NFS traffic from EKS nodes.

4. Install and deploy the vLLM stack using Helm.

5. Validate the setup by checking the status of the pods.
[[Full Tutorial]](https://github.com/vllm-project/production-stack/blob/main/tutorials/cloud_deployments/01-AWS-EKS-deployment.md)

## Deploying on Google Cloud Platform (GCP) with GKE

To run the service, go to "deployment_on_cloud/gcp" and run:

```bash
sudo bash entry_point.sh YAML_FILE_PATH
```

Pods for the vllm deployment should transition to Ready and the Running state.

Expected output:

```plaintext
NAME                                            READY   STATUS    RESTARTS   AGE
vllm-deployment-router-69b7f9748d-xrkvn         1/1     Running   0          75s
vllm-opt125m-deployment-vllm-696c998c6f-mvhg4   1/1     Running   0          75s
```

Clean up the service with:

```bash
bash clean_up.sh production-stack
```
The script will:
1. Set up an GKE cluster with GPU nodes.

2. Install and deploy the vLLM stack using Helm.
[[Full Tutorial]](https://github.com/vllm-project/production-stack/blob/main/tutorials/cloud_deployments/02-GCP-GKE-deployment.md)

## Conclusion
We release this tutorial to enable running vLLM production-stack on AWS EKS and GCP GKE platform from scratch. More features including Terraform deployment and AWS autoscaling are on its way. The next blog will be serving multiple models within the same K8S cluster. 

Stay tuned and tell us what we should do next! [[Poll]](https://forms.gle/QdBhpCPzv8tMP7QJA)

Join us to build a future where every application can harness the power of LLM inference—reliably, at scale, and without breaking a sweat.
*Happy deploying!*

Contacts:
- **Github: [https://github.com/vllm-project/production-stack](https://github.com/vllm-project/production-stack)**
- **Chat with the Developers** **[Interest Form](https://forms.gle/mQfQDUXbKfp2St1z7)**
- **vLLM [slack](https://slack.vllm.ai/)**
- **LMCache [slack](https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ)**
