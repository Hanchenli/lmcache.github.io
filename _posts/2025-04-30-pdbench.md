---
layout: post
title: "vLLM v1 + LMCache + NIXL = Prefill-Decode Disaggregation at State-Of-The-Art Speed"
thumbnail-img: /assets/img/tencent_blog/lmcache-tencent.jpg
share-img: /assets/img/tencent_blog/lmcache-tencent.jpg
author: LMCache Team
image: /assets/img/tencent_blog/lmcache-tencent.jpg
---
<br>

**TL;DR:**  
In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we introduced LMCache’s integration with vLLM v1 and NVIDIA’s NIXL, enabling *Prefill-Decode Disaggregation* (PD) for LLM inference. Today, we’re excited to share benchmark results that confirm this system achieves **state-of-the-art PD performance**, balancing *time-to-first-token* (TTFT) and *inter-token latency* (ITL) with unprecedented consistency.

---

## Why This Matters: LLM Inference on the Critical Path

Large language models are no longer confined to offline batch tasks—they are increasingly on the *critical path* of real-time, latency-sensitive applications like:

- **Real-time copilots** in IDEs (e.g., GitHub Copilot, Cursor)
- **Voice assistants** and smart reply systems
- **Conversational agents** for customer support (e.g., Intercom, Ada)
- **Realtime search and retrieval augmentation** (RAG) systems

In these settings, it's not enough for a model to *eventually* return results—the generation process needs to be *fast* and *smooth*. The two most important latency metrics are:

- **Time-to-first-token (TTFT):** How fast the model starts responding
- **Inter-token latency (ITL):** How consistently fast the tokens are streamed afterward

In short, TTFT helps improve perceived responsiveness. ITL ensures that long-form completions don’t lag or stall midway. And for enterprise-grade deployments, both must be tightly bounded to meet service-level objectives (SLOs).

---

## Enter Prefill-Decode (PD) Disaggregation

A promising solution to this problem, **Prefill-Decode (PD) Disaggregation**, emerged from Microsoft Research in 2023 and has since become the default architecture in many cutting-edge inference stacks. The core idea is to **separate the "prefill" and "decode" stages** of inference into different processes, allowing decoders to stream tokens without being blocked by ongoing prefill jobs.

Why does PD matter?

- Without PD: New requests preempt ongoing decoding, leading to high tail latencies for long-form completions.
- With PD: Decoders are dedicated to generating tokens, leading to smooth, uninterrupted streaming.

PD is now adopted in several production-grade inference engines and internal deployments at top-tier AI platforms.

---

## The Gap in vLLM v1 — Until Now

**vLLM** is arguably the most popular open-source inference engine for LLMs, known for its *paged attention* and *efficient scheduling*. With the release of **vLLM v1**, major improvements were introduced, such as chunked prefill and smarter memory management.

Yet, PD was missing from the official vLLM stack—especially the *XpYd* setup (X prefillers, Y decoders), which is widely used in production to maximize multi-node throughput.

This is where **LMCache** enters.

---

## LMCache + vLLM v1 + NIXL = PD at State-of-the-Art Speed

In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we announced LMCache’s integration with vLLM v1 and NVIDIA’s NIXL to support Disaggregated Prefill.

Today, we present the benchmark results that validate its performance.

### Set up #1: 1P1D

- **Topology:** 1 prefillers + 1 decoder (1p1d) on 2-GPU node with NVLink
- **Workload:** 3.6 queries per second, each with 8000 input tokens (prefill) + 200 output tokens (decode)  

Delay numbers

<img width="500" alt="image" src="https://github.com/user-attachments/assets/ea3ba4ca-1383-4965-9760-e1cb3e60efd4" />


Bar chart visualization

<img width="500" alt="image" src="https://github.com/user-attachments/assets/be41780e-6346-4963-973e-e0a4db2f0b2f" />



### Set up #2: 2P1D

- **Topology:** 2 prefillers + 1 decoder (1p1d) on 3-GPU node with NVLink
- **Workload:** 5.5 queries per second, each with 10000 input tokens (prefill) + 100 output tokens (decode)  

Delay numbers

<img width="500" alt="image" src="https://github.com/user-attachments/assets/fbbed199-8685-4cdb-be46-802d84c504ba" />


Bar chart visualization

<img width="500" alt="image" src="https://github.com/user-attachments/assets/942a2c08-f94f-4bf0-9e41-9f4eae1cc291" />


### Key takeaway:  
> **LMCache+vLLM v1 outperforms vLLM (w/o PD) and achieves similar performance as Dynamo across TTFT and ITL**

---

## Design Deep Dive: How We Achieve It

The core challenge of any PD system lies in **efficiently transferring the KV cache** between prefillers and decoders.

Here’s how we do it:

1. **KV cache is extracted from vLLM’s paged memory**
2. **LMCache collects and assembles the KV into a GPU-resident contiguous buffer**
3. **The buffer is sent over the network via NIXL to decoders**

<img width="823" alt="image" src="https://github.com/user-attachments/assets/bdd667eb-719e-42d8-86fe-434c3a639300" />


### Why Use a Buffer?

At first glance, adding a buffer sounds like extra overhead. But let’s consider the alternative: sending KV cache *block by block* directly from vLLM’s paged memory. With a default block size of 16 tokens, this results in **thousands of tiny transfers**, each underutilizing bandwidth—even with NIXL.

In contrast, **LMCache batches the KV blocks into a single large buffer**, achieving:

- High GPU-to-NIC throughput
- Minimal fragmentation
- Near-zero memory copy overhead (since it’s all in GPU memory)

It’s analogous to how **OSes use I/O buffers** to reduce syscall overhead and improve disk/network performance.

### Why Page Size Matters

In Dynamo, performance varies significantly with vLLM’s page size. A smaller page means more tiny KV transfers; a larger page introduces memory fragmentation and hurts prefix caching.

LMCache solves this by buffering at send-time, **decoupling network transfer efficiency from page size**.

Dynamo's 
- Page size: 16 tokens

*Why Not Just Increase Page Size in vLLM?*

Great question.

vLLM sets a **default block size of 16 tokens** for a reason—**prefix caching**. Larger blocks reduce prefix hit rates unless complex partial-matching mechanisms are added. Moreover, large blocks fragment GPU memory, clashing with the paged attention philosophy.

Again, this is similar to OS memory paging—smaller pages allow fine-grained caching and less fragmentation.

By leveraging **LMCache’s decoupled buffering**, we get the best of both worlds:
- Retain small pages for better memory usage and prefix hits
- Still achieve efficient KV transfer for PD

---

## Final Thoughts

Disaggregated Prefill is not just a nice-to-have—it’s becoming **foundational** to high-performance LLM inference.

With LMCache’s PD support for vLLM v1 + NIXL, we bring **production-grade PD to open-source stacks**, with **state-of-the-art performance** and a robust, future-proof architecture.

In all fairness, the 

Stay tuned—we're just getting started. 

---

Would you like help generating the diagrams or benchmarking visuals for this blog?
