---
layout: post
title: "vLLM v1 + LMCache + NIXL = Disaggregated Prefill at State-Of-The-Art Speed"
thumbnail-img: /assets/img/tencent_blog/lmcache-tencent.jpg
share-img: /assets/img/tencent_blog/lmcache-tencent.jpg
author: LMCache Team
image: /assets/img/tencent_blog/lmcache-tencent.jpg
---
<br>

**TL;DR:**  
In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we introduced LMCache’s integration with vLLM v1 and NVIDIA’s NIXL, enabling *Disaggregated Prefill* (DP) for LLM inference. Today, we’re excited to share benchmark results that confirm this system achieves **state-of-the-art DP performance**, balancing *time-to-first-token* (TTFT) and *inter-token latency* (ITL) with unprecedented consistency.

---

## Why This Matters: LLM Inference on the Critical Path

Large language models are no longer confined to offline batch tasks—they are increasingly on the *critical path* of real-time, latency-sensitive applications like:

- **Real-time copilots** in IDEs (e.g., GitHub Copilot, Cursor)
- **Voice assistants** and smart reply systems
- **Conversational agents** for customer support (e.g., Intercom, Ada)
- **Realtime search and retrieval augmentation** (RAG) systems

In these settings, it's not enough for a model to *eventually* return results—it needs to be *fast* and *consistent*. The two most important latency metrics are:

- **Time-to-first-token (TTFT):** How fast the model starts responding
- **Inter-token latency (ITL):** How consistently fast the tokens are streamed afterward

TTFT helps improve perceived responsiveness. ITL ensures that long-form completions don’t lag or stall midway. And for enterprise-grade deployments, both must be tightly bounded to meet service-level objectives (SLOs).

---

## Enter Disaggregated Prefill (DP)

A promising solution to this problem, **Disaggregated Prefill (DP)**, emerged from Microsoft Research in 2023 and has since become the default architecture in many cutting-edge inference stacks. The core idea is to **separate the "prefill" and "decode" stages** of inference into different processes, allowing decoders to stream tokens without being blocked by ongoing prefill jobs.

Why does DP matter?

- Without DP: New requests preempt ongoing decoding, leading to high tail latencies for long-form completions.
- With DP: Decoders are dedicated to generating tokens, leading to smooth, uninterrupted streaming.

DP is now adopted in several production-grade inference engines, including [Azure’s vLLM backend](https://www.microsoft.com/en-us/research/blog/accelerating-llm-serving-with-disaggregated-prefill/) and internal deployments at top-tier AI platforms.

---

## The Gap in vLLM v1 — Until Now

**vLLM** is arguably the most popular open-source inference engine for LLMs, known for its *paged attention* and *efficient scheduling*. With the release of **vLLM v1**, major improvements were introduced, such as chunked prefill and smarter memory management.

Yet, DP was missing from the official vLLM stack—especially the *XpYd* setup (X prefillers, Y decoders), which is widely used in production to maximize multi-node throughput.

This is where **LMCache** enters.

---

## LMCache + vLLM v1 + NIXL = DP at State-of-the-Art Speed

In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we announced LMCache’s integration with vLLM v1 and NVIDIA’s NIXL to support Disaggregated Prefill.

Today, we present the benchmark results that validate its performance.

### Experimental Setup:

- **Topology:** 3 prefillers + 1 decoder (3p1d) on 4 GPU nodes  
- **Workload:** 5 QPS, each with 1000 input tokens (prefill) + 100 output tokens (decode)  
- **Baselines:**  
   - *vLLM v1* (no DP)  
   - *Dynamo* (reference DP framework)

### Results:

![image](https://github.com/user-attachments/assets/16a50790-3662-44f4-8cdf-5930c6fcd940)


Key takeaway:  
> **LMCache+vLLM v1 outperforms vLLM (w/o DP) and achieves similar performance as Dynamo across TTFT and ITL**

---

## Design Deep Dive: How We Achieve It

The core challenge of any DP system lies in **efficiently transferring the KV cache** between prefillers and decoders.

Here’s how we do it:

1. **KV cache is extracted from vLLM’s paged memory**
2. **LMCache collects and assembles the KV into a GPU-resident contiguous buffer**
3. **The buffer is sent over the network via NIXL to decoders**

<img width="823" alt="image" src="https://github.com/user-attachments/assets/bdd667eb-719e-42d8-86fe-434c3a639300" />


### Why Use a Buffer?

At first glance, adding a buffer sounds like extra overhead. But let’s consider the alternative: sending KV cache *block by block* directly from vLLM’s paged memory. With a default block size of 16 tokens, this results in **thousands of tiny transfers**, each underutilizing bandwidth—even with NIXL.

In contrast, **LMCache batches the KV blocks into a single large buffer**, achieving:

- High GPU-to-NIC throughput
- Minimal fragmentation
- Near-zero memory copy overhead (since it’s all in GPU memory)

It’s analogous to how **OSes use I/O buffers** to reduce syscall overhead and improve disk/network performance.

### Benchmark: Why Page Size Matters

In Dynamo, performance varies significantly with vLLM’s page size. A smaller page means more tiny KV transfers; a larger page introduces memory fragmentation and hurts prefix caching.

LMCache solves this by buffering at send-time, **decoupling network transfer efficiency from page size**.

*(Figure Placeholder – Page size sensitivity comparison between Dynamo and LMCache)*

---

## Why Not Just Increase Page Size in vLLM?

Great question.

vLLM sets a **default block size of 16 tokens** for a reason—**prefix caching**. Larger blocks reduce prefix hit rates unless complex partial-matching mechanisms are added. Moreover, large blocks fragment GPU memory, clashing with the paged attention philosophy.

Again, this is similar to OS memory paging—smaller pages allow fine-grained caching and less fragmentation.

By leveraging **LMCache’s decoupled buffering**, we get the best of both worlds:
- Retain small pages for better memory usage and prefix hits
- Still achieve efficient KV transfer for DP

---

## Final Thoughts

Disaggregated Prefill is not just a nice-to-have—it’s becoming **foundational** to high-performance LLM inference.

With LMCache’s DP support for vLLM v1 + NIXL, we bring **production-grade DP to open-source stacks**, with **state-of-the-art performance** and a robust, future-proof architecture.

Stay tuned—we're just getting started.

---

Would you like help generating the diagrams or benchmarking visuals for this blog?
