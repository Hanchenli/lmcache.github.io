---
layout: post
title: "You focus on KV cache research, we make it compatible with vLLM"
thumbnail-img: /assets/img/lmcache_research.png
share-img: /assets/img/lmcache_research.png
author: LMCache Team
image: /assets/img/lmcache_research.png
---
<br>

<div align="center">
<img src="/assets/img/lmcache_research.png" alt="Icon" style="width: 80%; vertical-align:middle;">
</div>

ðŸš€ Building your system on KV cache? Try building it on [LMCache](https://github.com/LMCache/LMCache)! 

By using LMCache for your research, you can focus on KV cache management, while we handle all the vLLM integration and compatibility for you.


Hereâ€™s why LMCache works as your research testbed:
- Simple codebase, all vLLM functionalities. vLLM has over 200k lines of Python code. LMCache? Just 7k lines, specialized for KV cache, so you get much less code to read and work with, while LMCache handles all the vLLM feature compatibility for you.
- Boost your evaluations with ready-made KV cache research. With implementations of the latest research (think [CacheGen](https://arxiv.org/pdf/2310.07240) and [CacheBlend](https://arxiv.org/pdf/2405.16444)), youâ€™re all set to use them as baselines or show that your system can be used together with them for higher improvements.
- Showcase your impact, fast. Implement in LMCache, and your research is instantly runnable with vLLM. Plus, we have rich demos for you to showcase your system in conference talks or presentations.

Check our [codebase](https://github.com/LMCache/LMCache) and [documentations](https://docs.lmcache.ai/) for more information!

Update: we are delighted to see that there are already papers (like [Cake](https://arxiv.org/pdf/2410.03065v1)) start to use LMCache as their research testbed.