---
layout: post
title: "LMCache: Turboboosting vLLM with 7x faster access to 100x more KV caches"
thumbnail-img: /assets/img/lmcache-blog1.png
share-img: /assets/img/lmcache-blog1.png
author: LMCache Team
image: /assets/img/lmcache-blog1.png
---
<br>

<!-- ------------ -->

<!-- <span style="font-size:1.3em; color=red;">**TL;DR:** <img src="/assets/img/lmcache-logo-small.png" alt="Icon" style="width:150px; vertical-align:middle;"> *turboboosts vLLM with <span style="font-size:1.3em; color=red;">**7Ã—**</span> faster access to <span style="font-size:1.3em; color=red;">**100Ã—**</span> more KV caches.*</span> -->



<span style="font-size:1.3em;">**TL;DR:** LMCache turboboosts vLLM with **<span style="color:#2AA198;">7Ã—</span>** faster access to **<span style="color:#2AA198;">100x</span>** more KV caches, for both multi-turn conversation **<span style="color:#2AA198;"> and RAG</span>** </span>.
<!-- 
<span style="font-size:1.3em; color=red;">**TL;DR:** LMCache turboboosts vLLM with **7Ã—** faster access to **100x** more KV caches.</span> -->

<!-- **TL; DR:** LMCache allows vLLM to access 100x more KV caches 7x faster by storing the compressed KV cache on disk! You can also reuse KV caches in RAG pipeline. -->

<br>

**[[Code]](https://github.com/LMCache/LMCache) [[Paper1]](https://arxiv.org/abs/2310.07240) [[Paper2]](https://arxiv.org/abs/2405.16444)**


LLMs are ubiquitous across industries, but when using them with long documents, *it takes forever for the model even to spit out the first token*. 

Meet **LMCache**, a new *open-source* system developed in UChicago that generates the first token 3-10x faster than vLLM by efficiently managing long documents (and other sources of knowledge) for you.


<div align="center">
<img src="/assets/img/lmcache-diagram.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

<br>
<br>

## What is LMCache?

No matter how smart LLMs become, it is always slow and costly for them to read external texts, videos, etc. LMCache reduces this cost by letting LLMs _**read each text only once**_. 
Our insight is that most data are _repetitively read_, such as popular books, chat history, and news documents. (Recall the Pareto's Rule: 20% of knowledge is used 80% of the time.)

By storing the KV caches (the LLM-usable representation) of all reusable texts, LMCache can reuse the KV caches of _**any**_ reused text (not necessarily the prefix) in _**any**_ serving engine instance. 
Developed at UChicago, this solution is already gaining significant interest from industry partners.

By combining LMCache with vLLM, LMCache reduces the "time to the first token" (TTFT) by 3-10x and saves the GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.

<div align="center">
<img src="/assets/img/lmcache-gain-short.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

<br>
<br>

## The Secret Sauces

This table contrasts vLLM with vs. without LMCache:

<div align="center">
<img src="/assets/img/lmcache-contrast.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

At its core, LMCache incorporated two recent research projects from U Chicago.

- **CacheGen [[SIGCOMM'24]](https://arxiv.org/abs/2310.07240)** efficiently encodes KV caches into bitstreams and stores them on disks.
This allows unlimited amount of KV caches to be stored on cheap disks and be shared across multiple vLLM instances. 
It is in contrast with vLLM which stores KV caches only in one LLM instance's internal GPU and CPU memory.
This capability is pressingly needed in multiple-round chat apps that use many vLLM instances to serve many users. 


- **CacheBlend [[EuroSys'25]](https://arxiv.org/abs/2405.16444)** blends KV caches with minimum computation to preserve cross-attention. 
This allows multiple KV caches to be composed together to dynamically form a new KV cache.
In contrast, vLLM only reuses the KV cache of the input prefix.
This is particularly useful for RAG applications where multiple reused texts constitute the LLM input.

<!--- CacheGen [SIGCOMM'24](https://arxiv.org/abs/2310.07240): A KV-cache compression system that encodes KV caches into compact bitstreams.-->
<!--- CacheBlend [EuroSys'25](https://arxiv.org/abs/2405.16444): A KV-cache blending system that dynamically composes new KV caches from smaller ones.-->

<br>
<br>

## Get Started with LMCache

We provide a docker-based quickstart demo that lets you start a serving engine (vLLM) with LMCache and then query the serving engine with a long context.

To begin with, you need to have a GPU server with the [docker environment](https://docs.docker.com/engine/install/) plus [nvidia-runtime](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed. 


To start the serving engine
```bash
git clone https://github.com/LMCache/LMCache && cd LMCache/examples && bash quickstart.sh
```

The serving engine is ready after you see the following lines in the log:
<div align="center">
<img src="/assets/img/lmcache-install-info.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

Now, go to `LMCache/examples` and run the following command to start the query front end:
```bash
pip install openai streamlit
streamlit run quickstart-frontend.py
```

You should be able to access the frontend from your browser at `http://<your server's IP>:8501`

The first query has a long TTFT because the server needs to prefill the long context. But once the first query finishes, the TTFT of all future queries will be much lower as LMCache shares the KV cache to vLLM which can then skip the prefill of the long context.

We also provide multiple demos at [ðŸ”—LMCache-demos repo](https://github.com/LMCache/demo). The demos cover the following use cases:
- Share KV caches across multiple serving engines [(ðŸ”—link)](https://github.com/LMCache/demo/tree/master/demo2-multi-node-sharing)
- Loading non-prefix KV caches for RAG [(ðŸ”—link)](https://github.com/LMCache/demo/tree/master/demo3-KV-blending)

<br>
<be>

## Contact Us

Interested? Shoot us an email at [lmcacheteam@gmail.com](mailto:lmcacheteam@gmail.com)!

