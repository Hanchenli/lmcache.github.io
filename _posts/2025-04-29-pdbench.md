---
layout: post
title: "Bringing State-Of-The-Art PD Speed to vLLM with LMCache"
thumbnail-img: https://github.com/user-attachments/assets/1d84ab03-6ba2-4518-b195-28e46471f9f1
share-img: https://github.com/user-attachments/assets/1d84ab03-6ba2-4518-b195-28e46471f9f1
author: LMCache Team
image: https://github.com/user-attachments/assets/1d84ab03-6ba2-4518-b195-28e46471f9f1

---
<br>

**TL;DR:**  
In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we introduced **[LMCache](https://github.com/LMCache/LMCache)**’s integration with **vLLM v1** and **NVIDIA’s NIXL** used in [Dynamo](https://github.com/ai-dynamo/dynamo), enabling *Prefill-Decode Disaggregation* (PD) for LLM inference. Today, we’re excited to share benchmark results that confirm this system achieves **state-of-the-art PD performance**, balancing *time-to-first-token* (TTFT) and *inter-token latency* (ITL) with unprecedented consistency.

Here's an example result (scroll down for more details and results):


<img width="300" alt="image" src="https://github.com/user-attachments/assets/1d84ab03-6ba2-4518-b195-28e46471f9f1" />




---

## Why This Matters: LLM Inference on the Critical Path

Large language models are no longer confined to offline batch tasks—they are increasingly on the *critical path* of real-time, latency-sensitive applications like:

- **Real-time copilots** in IDEs (e.g., GitHub Copilot, Cursor)
- **Voice assistants** and smart reply systems
- **Conversational agents** for customer support (e.g., Intercom, Ada)
- **Realtime search and retrieval augmentation** (RAG) systems

In these settings, it's not enough for a model to *eventually* return results—the generation process needs to be *fast* and *smooth*. The two most important latency metrics are:

- **Time-to-first-token (TTFT):** How fast the model starts responding
- **Inter-token latency (ITL):** How consistently fast the tokens are streamed afterward

In short, TTFT helps improve perceived responsiveness. ITL ensures that long-form completions don’t lag or stall midway. And for enterprise-grade deployments, both must be tightly bounded to meet service-level objectives (SLOs).

---

## Enter Prefill-Decode (PD) Disaggregation

A promising solution to this problem, **Prefill-Decode (PD) Disaggregation**, emerged from Microsoft Research in 2023 and has since become the default architecture in many cutting-edge inference stacks. The core idea is to **separate the "prefill" and "decode" stages** of inference into different processes, allowing decoders to stream tokens without being blocked by ongoing prefill jobs.

Why does PD matter?

- Without PD: New requests preempt ongoing decoding, leading to high tail latencies for long-form completions.
- With PD: Decoders are dedicated to generating tokens, leading to smooth, uninterrupted streaming.

PD is now adopted in several production-grade inference engines and internal deployments at top-tier AI platforms.

---

## The Gap in vLLM v1 — Until Now

**vLLM** is arguably the most popular open-source inference engine for LLMs, known for its *paged attention* and *efficient scheduling*. With the release of **vLLM v1**, major improvements were introduced, such as chunked prefill and smarter memory management.

Yet, PD was missing from the official vLLM stack—especially the *XpYd* setup (X prefillers, Y decoders), which is widely used in production to maximize multi-node throughput.

This is where **LMCache** enters.

In our [previous blog](https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/), we announced LMCache’s integration with vLLM v1 and NVIDIA’s NIXL to support Disaggregated Prefill.

**LMCache's upstream support in vLLM**
- *LMCache connector merged into vLLM:* https://github.com/vllm-project/vllm/pull/16625
- *LMCache NIXL integration:* https://github.com/LMCache/LMCache/pull/446
- *LMCache + NIXL XpYd support:* https://github.com/LMCache/LMCache/pull/528
- *Quickstart documentation:* https://docs.lmcache.ai/getting_started/quickstart/disaggregated_prefill.html

---

## LMCache + vLLM v1 + NIXL = PD at State-of-the-Art Speed

Today, we present the benchmark results that validate its performance.

### Set up #1: 1P1D

- **Topology:** 1 prefillers + 1 decoder (1p1d) on 2-GPU node with NVLink
- **Workload:** 3.6 queries per second, each with 8000 input tokens (prefill) + 200 output tokens (decode)  


<!-- <img width="500" alt="image" src="https://github.com/user-attachments/assets/ea3ba4ca-1383-4965-9760-e1cb3e60efd4" /> -->

Results: 

<img width="600" alt="image" src="https://github.com/user-attachments/assets/be41780e-6346-4963-973e-e0a4db2f0b2f" />



### Set up #2: 2P1D

- **Topology:** 2 prefillers + 1 decoder (1p1d) on 3-GPU node with NVLink
- **Workload:** 5.5 queries per second, each with 10000 input tokens (prefill) + 100 output tokens (decode)  

<!-- <img width="500" alt="image" src="https://github.com/user-attachments/assets/fbbed199-8685-4cdb-be46-802d84c504ba" /> -->


Results: 

<img width="600" alt="image" src="https://github.com/user-attachments/assets/942a2c08-f94f-4bf0-9e41-9f4eae1cc291" />


### Key takeaway:  
> **vLLM v1 (w/ PD) outperforms vLLM (w/o PD) and achieves similar performance as Dynamo across TTFT and ITL**

---

## Design Deep Dive: How We Achieve It

The core challenge of any PD system lies in **efficiently transferring the KV cache** between prefillers and decoders.

Here’s how we do it:

1. **KV cache is extracted from vLLM’s paged memory**
2. **LMCache collects and assembles the KV into a GPU-resident contiguous buffer**
3. **The buffer is sent over the network via NIXL to decoders**

<img width="823" alt="image" src="https://github.com/user-attachments/assets/9faaf36c-2a01-4963-8325-15cff4e3e660" />



### Why Use a Buffer?

At first glance, adding a buffer sounds like extra overhead. But let’s consider the alternative: sending KV cache *block by block* directly from vLLM’s paged memory. With a default block size of 16 tokens, this results in **many tiny transfers**, each underutilizing bandwidth—even with NIXL.

In contrast, **LMCache batches the KV blocks into a single large buffer**, achieving:

- High GPU-to-NIC throughput
- Minimal fragmentation
- Near-zero memory copy overhead (since it’s all in GPU memory)

It’s analogous to how **OSes use I/O buffers** to reduce syscall overhead and improve disk/network performance.

### Why Page Size Matters

In Dynamo, performance varies significantly with vLLM’s page size. A smaller page means more tiny KV transfers; a larger page introduces memory fragmentation and hurts prefix caching.

LMCache solves this by buffering at send-time, **decoupling network transfer efficiency from page size**.

Dynamo's delay to transfer the KV cache of a 7500-token input through NVLink
- **20ms**, if page size is **16** tokens
- **8ms**, if page size is **128** tokens

*Why Not Just Increase Page Size in vLLM?*

Great question.

vLLM sets a **default block size of 16 tokens** for a reason—**prefix caching**. Larger blocks reduce prefix hit rates unless complex partial-matching mechanisms are added. Moreover, large blocks fragment GPU memory, clashing with the paged attention philosophy.

Again, this is similar to OS memory paging—smaller pages allow fine-grained caching and less fragmentation.

By leveraging **LMCache’s decoupled buffering**, we get the best of both worlds:
- Retain small pages for better memory usage and prefix hits
- Still achieve efficient KV transfer for PD

---

## Final Notes

Disaggregated Prefill is not just a nice-to-have—it’s becoming **foundational** to high-performance LLM inference.

With LMCache’s PD support for vLLM v1 + NIXL, we bring **production-grade PD to open-source stacks**, with **state-of-the-art performance** and a robust, future-proof architecture.

In all fairness, we don't think LMCache's design is optimal. The LMCache Lab works closely with vLLM team to explore better designs of PD disaggregation.

Stay tuned—we're just getting started. 

