---
layout: post
title: "ðŸ“– Explore LMCache Documentation"
thumbnail-img: /assets/img/blog-snapshot.png
share-img: /assets/img/blog-snapshot.png
author: LMCache Team
image: /assets/img/blog-snapshot.png
---
<br>

<div align="center">
<img src="/assets/img/blog-snapshot.png" alt="Icon" style="width: 80%; vertical-align:middle;">
</div>


Weâ€™re excited to announce that our [**LMCache documentation**](https://docs.lmcache.ai) is now live! ðŸŽ‰

This documentation website to help you get started quickly and understand all the key features. Hereâ€™s what youâ€™ll find:

- **Getting Started:** Step-by-step installation and setup guides to get LMCache up and running with your LLM projects.
- **Examples and Use Cases:** Practical examples for integrating LMCache with vLLM, including demos on multi-instance cache sharing and RAG (Retrieval-Augmented Generation).
- **Developer Guide:** In-depth details on configuration, advanced usage, and how to customize LMCache for your specific needs.

Our documentation is designed for **both beginners and experienced developers** who want to optimize LLM inference and explore cutting-edge techniques. [Check out the documentation here](https://docs.lmcache.ai) and take your first steps towards more efficient AI deployment!

Stay tuned for more updates and detailed documentation coming soon. 
